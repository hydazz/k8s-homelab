apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: open-webui
  namespace: open-webui
spec:
  interval: 5m
  install:
    createNamespace: true
    crds: CreateReplace
    remediation:
      retries: 3
  upgrade:
    crds: CreateReplace
    remediation:
      retries: 3
  chart:
    spec:
      chart: open-webui
      version: 6.9.0
      interval: 5m
      sourceRef:
        kind: HelmRepository
        name: open-webui
        namespace: flux-system
  values:
    image:
      repository: ghcr.io/open-webui/open-webui
      tag: v0.6.7
    ollama:
      fullnameOverride: ollama
      image:
        repository: ollama/ollama
        tag: 0.6.8
      ingress:
        enabled: true
        className: internal-nginx
        hosts:
          - host: ollama.k8s.hyde.services
            paths:
              - path: /
                pathType: Prefix
        tls:
          - secretName: wildcard.hyde.services
            hosts:
              - ollama.k8s.hyde.services
      resources:
        requests:
          cpu: 10m
          memory: 256Mi
        limits:
          cpu: 500m
          memory: 6Gi
      nodeSelector:
        nvidia.com/gpu.present: "true"
      ollama:
        gpu:
          enabled: true
          type: "nvidia"
          number: 1
          nvidiaResource: "nvidia.com/gpu"
        models:
          pull:
            - llama3.1
      runtimeClassName: nvidia
      persistentVolume:
        enabled: true
        size: 16Gi
    podAnnotations:
      backup.velero.io/backup-volumes: data
    ingress:
      enabled: true
      class: external-nginx
      host: chat.hyde.services
      tls: true
      existingSecret: wildcard.hyde.services
    persistence:
      enabled: true
      size: 2Gi
